{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from playsound import playsound\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change working directory to get the file\n",
    "try:\n",
    "    os.chdir('/Users/pkg/Documents/OneDrive - Elektroforeningen (EFO)/Python/Intermediate DS')\n",
    "except:\n",
    "    os.chdir('/Users/paal/OneDrive - Elektroforeningen (EFO)/Python/Intermediate DS')\n",
    "\n",
    "# Open pickled file from the data wrangling section and set working directory\n",
    "with open('data_wrangling.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "try:\n",
    "    os.chdir('/Users/pkg/Documents/OneDrive - Elektroforeningen (EFO)/Python/Intermediate DS/Intermediate Data Science with Python/Python_Capstone')\n",
    "except:\n",
    "    os.chdir('/Users/paal/OneDrive - Elektroforeningen (EFO)/Python/Intermediate DS/Intermediate Data Science with Python/Python_Capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort by df by ETIM_classes for the clf.predict_proba-method later\n",
    "df = df.sort_values(['ETIM_class', 'ENG'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the labels from the rest of the data set\n",
    "labels = df['ETIM_class']\n",
    "ex_var = df[['ENG', 'Technical_description']]\n",
    "\n",
    "# Display some attributes of the dataset\n",
    "print(\"labels'  shape:\", labels.shape)\n",
    "print(\"ex_var's shape:\", ex_var.shape)\n",
    "print(\"\")\n",
    "print(\"labels is of type\", type(labels))\n",
    "print(\"ex_var is of type\", type(ex_var))\n",
    "print(\"\")\n",
    "print(\"first label after sorting:\", labels.iloc[0])\n",
    "print(\"first ENG after sorting:\", ex_var.iloc[0,0])\n",
    "print(\"first technical description after sorting:\", ex_var.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing stop words that will be ignored\n",
    "stop_words = get_stop_words('norwegian')\n",
    "\n",
    "# Creating a Pandas Series of the technical descriptions\n",
    "text = df['Technical_description']\n",
    "\n",
    "# Creating the corpus\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=2)\n",
    "\n",
    "# Build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# Convert text to a bag of words, returns a Compressed Sparse Row matrix\n",
    "# This is suitable for a matrix that is primarily made up of zeroes.\n",
    "x = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up X and y\n",
    "X = x\n",
    "y = labels\n",
    "\n",
    "# Create the test and training sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the classifier over the training set, and test on the test set\n",
    "clf = MultinomialNB(alpha=0.0000000001).fit(xtrain, ytrain)\n",
    "#NB_train_accuracy = clf.score(xtrain, ytrain)\n",
    "#NB_test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "# Accuracy scores for both the training and test sets\n",
    "#print(\"Training accuracy:\", round(NB_train_accuracy, 2))\n",
    "#print(\"Testing accuracy\", round(NB_test_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the classification report\n",
    "#y_pred_train = clf.predict(xtrain)\n",
    "#y_pred_test = clf.predict(xtest)\n",
    "\n",
    "#print(classification_report(y_pred_train, ytrain))\n",
    "#print(classification_report(y_pred_test, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to connect the ETIM-class-codes and the descriptions (in Norwegian)\n",
    "ETIM_dict = pd.read_csv('ETIM7.csv', header=None, sep=';', index_col=0).to_dict()[1]\n",
    "#ETIM_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for single technical descriptionsvestre v\n",
    "inputText = widgets.Text(value='Fyll inn teknisk beskrivelse her')\n",
    "\n",
    "# Create query\n",
    "def predict_ETIM(sender):\n",
    "\n",
    "    # Creating technical description to be evaluated by the model\n",
    "    text = inputText.value\n",
    "    example = clf.predict_proba(vectorizer.transform([text])).flatten()\n",
    "\n",
    "    # Display the predicted class\n",
    "    test = clf.predict(vectorizer.transform([text]))\n",
    "    #test[0]\n",
    "\n",
    "    # Create a dictionary for  the classes and probabilities\n",
    "    example_dict = dict(zip(sorted(ytrain.unique()), example))\n",
    "    #example_dict\n",
    "\n",
    "    # Display the top 3 predictions, with classes and probabilities\n",
    "    top3 = {k: example_dict[k] for k in sorted(example_dict, key=example_dict.__getitem__, reverse=True)[:3]}\n",
    "    #top3[test[0]]\n",
    "\n",
    "    # Shorten to integer percentage\n",
    "    #\"%.0f\" % (top3[test[0]] * 100)\n",
    "    top3s = sorted(example_dict, key=example_dict.__getitem__, reverse=True)[:3]\n",
    "\n",
    "    print('Tekst:', text)\n",
    "    print('')\n",
    "    \n",
    "    for i in range(len(top3s)):\n",
    "\n",
    "        if top3[top3s[i]] < 0.1:\n",
    "            break\n",
    "        else:\n",
    "            print('Forslag', int(i+1), ':')\n",
    "            print('Forventet ETIM-klasse:', top3s[i], ETIM_dict[top3s[i]])\n",
    "            print('Forventet treffsikkerhet:', \"%.0f\" % (top3[top3s[i]] * 100), '%')\n",
    "            print('')\n",
    "    \n",
    "inputText.on_submit(predict_ETIM)\n",
    "inputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Excel-file with technical descriptions that need to be classified\n",
    "\n",
    "# Assign spreadsheet filename to 'file'\n",
    "file = 'import_mw.xlsx'\n",
    "\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile(file).parse('Sheet1')\n",
    "\n",
    "# See first technical description and length of file\n",
    "print(xl.iloc[0,:])\n",
    "len(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the analysis results\n",
    "rows_list = []\n",
    "\n",
    "# Predict ETIM-classes for a number of technical descriptions (for use in analyze_xl, not standalone use)\n",
    "# For standalone use, see predict_ETIM()\n",
    "def predict_ETIM_raw(elnr, text):\n",
    "    \n",
    "    # Creating technical description to be evaluated by the model\n",
    "    example = clf.predict_proba(vectorizer.transform([text])).flatten()\n",
    "\n",
    "    # Display the predicted class\n",
    "    single_prod = clf.predict(vectorizer.transform([text]))\n",
    "\n",
    "    # Create a dictionary for the classes and probabilities\n",
    "    prob_dict = dict(zip(sorted(ytrain.unique()), example))\n",
    "\n",
    "    # Top 3 predictions, with classes and probabilities\n",
    "    top3 = {k: prob_dict[k] for k in sorted(prob_dict, key=prob_dict.__getitem__, reverse=True)[:3]}\n",
    "\n",
    "    # Top 3 predictions in sorted order\n",
    "    top3s = sorted(prob_dict, key=prob_dict.__getitem__, reverse=True)[:3]\n",
    "\n",
    "    for i in range(len(top3)):\n",
    "        \n",
    "        single_prod = {elnr: [k for k in [text, top3s[i], ETIM_dict[top3s[i]], \"%.0f\" % (top3[top3s[i]] * 100)]]}\n",
    "        \n",
    "        if top3[top3s[i]] < 0.1:\n",
    "            break\n",
    "        else:\n",
    "            print(single_prod)\n",
    "            temp_dict = {}\n",
    "            temp_dict.update(single_prod)\n",
    "            rows_list.append(temp_dict)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the analysis on the data from the Excel-file\n",
    "def analyze_xl(file):\n",
    "    \n",
    "    # Find text from Excel-file to analyze\n",
    "    for j in range(len(xl)):\n",
    "        predict_ETIM_raw(xl.iloc[j,0], xl.iloc[j,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis (prints results and writes to rows_list)\n",
    "analyze_xl(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a dataframe\n",
    "output_df = pd.DataFrame(rows_list).T\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Squeeze the values to the left\n",
    "def squeeze_nan(x):\n",
    "    original_columns = x.index.tolist()\n",
    "\n",
    "    squeezed = x.dropna()\n",
    "    squeezed.index = [original_columns[n] for n in range(squeezed.count())]\n",
    "\n",
    "    return squeezed.reindex(original_columns, fill_value=np.nan)\n",
    "\n",
    "# Run the function and remove rightmost columns with only Nan\n",
    "# Nr. of columns need to be correct according to output_df above\n",
    "output_df = output_df.apply(squeeze_nan, axis=1)\n",
    "output_df = output_df.dropna(axis=1, how='all')\n",
    "output_df.columns = ['anbf1', 'anbf2', 'anbf3']\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatinate\n",
    "output_df1 = pd.concat([output_df['anbf1'], output_df['anbf1'].apply(pd.Series).add_prefix('nr_')], axis=1)\n",
    "output_df2 = pd.concat([output_df['anbf2'], output_df['anbf2'].apply(pd.Series).add_prefix('nr_')], axis=1)\n",
    "output_df3 = pd.concat([output_df['anbf3'], output_df['anbf3'].apply(pd.Series).add_prefix('nr_')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate files\n",
    "output_df = pd.concat([output_df1, output_df2, output_df3], axis=1)\n",
    "output_df.columns = output_df.columns.astype(str)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unconcatinated columns\n",
    "output_df = output_df.drop(['anbf1', 'anbf2', 'anbf3'], 1)\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "output_df.columns = ['Tekst', 'ETIM-kode', 'Navn', 'Forventet sannsynlighet', 'Tekst', 'ETIM-kode', 'Navn', 'Forventet sannsynlighet', 'Tekst', 'ETIM-kode', 'Navn', 'Forventet sannsynlighet']\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export the file to an Excel-file\n",
    "writer = pd.ExcelWriter('output_mw.xlsx', engine='xlsxwriter')\n",
    "#worksheet.conditional_format('')\n",
    "output_df.to_excel(writer, 'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "# Finished!\n",
    "playsound('fanfare_ff2.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
